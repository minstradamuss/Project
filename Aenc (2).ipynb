{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CcSEQiLMwI33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "01f49d90-a8ed-49c7-bd7c-abdfacb9480b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'segmentation_models_pytorch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b867c7deb208>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation_models_pytorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'segmentation_models_pytorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, Subset, random_split\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "import segmentation_models_pytorch as smp\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "UMijnTrmhzY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f5b6e9-5150-47b7-b503-b95b2a918b69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip\n",
        "%ls"
      ],
      "metadata": {
        "id": "QdRhB9UHwu_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc7399b-ec80-4a2a-fd3f-ffff31209667"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of data.zip or\n",
            "        data.zip.zip, and cannot find data.zip.ZIP, period.\n",
            "data.zip  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataset_dir, train=False):\n",
        "        self.data_transforms = {\n",
        "                            'train': transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Resize((256, 256)),\n",
        "                                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                            ]),\n",
        "                            'val': transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Resize((256, 256)),\n",
        "                                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                            ]),\n",
        "                        }\n",
        "        self.dataset_dir = dataset_dir\n",
        "\n",
        "        self.labels = []\n",
        "        self.raw_images = []\n",
        "        self.train = train\n",
        "\n",
        "        for dirpath, dirnames, filenames in os.walk(dataset_dir):\n",
        "            for fn in filenames:\n",
        "                label = 0 if 'normal' in dirpath else 1\n",
        "                raw_img = cv2.imread(f'{dirpath}/{fn}')\n",
        "                self.labels.append(label)\n",
        "                self.raw_images.append(raw_img)\n",
        "\n",
        "        self.train_images = [self.data_transforms['train'](img) for img in self.raw_images]\n",
        "        self.test_images = [self.data_transforms['val'](img) for img in self.raw_images]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.train:\n",
        "            return self.train_images[idx], self.labels[idx]\n",
        "        else:\n",
        "            return self.test_images[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "5cmxhcnaw5sM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data/'\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NibFX_BD1BL7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_images = 0\n",
        "for dirpath, dirnames, filenames in os.walk(data_dir):\n",
        "    n_images += len(filenames)"
      ],
      "metadata": {
        "id": "YoyfoJuW1FbH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyekYFLca7eT",
        "outputId": "ccecefba-2141-4d88-a9f4-5f85c1f51c09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_idxs = np.arange(n_images)"
      ],
      "metadata": {
        "id": "0M2tP9tF1RL9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageDataset(data_dir)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "2v2qZ5N91UR4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "b50b4511-183e-4965-af17-f36a26146d21"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b941a9408a8c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "b-2mBKYJc9Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = {0: 'normal', 1: 'phishing'}"
      ],
      "metadata": {
        "id": "AMeyk6Mm1bsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "O7VgCMb4iDb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inp, title=None):\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "inputs, classes = next(iter(dataloader))\n",
        "\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x.item()] for x in classes])"
      ],
      "metadata": {
        "id": "7ONfdkwwxQsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, image_size, channels, embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.image_size = image_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channels, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.shape_before_flattening = self.calculate_shape()\n",
        "        flattened_size = (image_size // 8) * (image_size // 8) * 128\n",
        "        self.fc = nn.Linear(flattened_size, embedding_dim)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def calculate_shape(self):\n",
        "        x = torch.zeros((1, self.channels, self.image_size, self.image_size))\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        return x.shape[1:]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NDDVrliIGYwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, shape_before_flattening, channels):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc = nn.Linear(embedding_dim, np.prod(shape_before_flattening))\n",
        "        self.reshape_dim = shape_before_flattening\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(\n",
        "            128, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
        "        )\n",
        "        self.deconv2 = nn.ConvTranspose2d(\n",
        "            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
        "        )\n",
        "        self.deconv3 = nn.ConvTranspose2d(\n",
        "            64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
        "        )\n",
        "\n",
        "        self.conv1 = nn.Conv2d(32, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), *self.reshape_dim)\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        x = F.relu(self.deconv3(x))\n",
        "        x = torch.sigmoid(self.conv1(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "VBbSIBa0GdUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, image_size, channels, embedding_dim, n_classes=1):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(image_size, channels, embedding_dim)\n",
        "        self.decoder = Decoder(embedding_dim, self.encoder.shape_before_flattening, channels)\n",
        "        self.classifier = nn.Linear(embedding_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.encoder(x)\n",
        "        faked = self.decoder(embeddings)\n",
        "        return faked\n",
        "\n",
        "    def get_embeddings(self, x):\n",
        "        embeddings = self.encoder(x)\n",
        "        return embeddings\n"
      ],
      "metadata": {
        "id": "ZFYH9SUpGrz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(num_epochs=25, model=None):\n",
        "    since = time.time()\n",
        "    sigm = nn.Sigmoid()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    best_model_params_path = os.path.join('.', 'autoencoder_best_model_params.pt')\n",
        "\n",
        "    best_loss = 1000\n",
        "\n",
        "    img_shape = dataset[0][0].shape\n",
        "    if model is None:\n",
        "        model = ConvAutoencoder(img_shape[1], img_shape[0], 1, 1)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    warmup_epochs = 5\n",
        "    warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: epoch / warmup_epochs)\n",
        "\n",
        "    main_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch: {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                loader = train_loader\n",
        "                model.train()\n",
        "            else:\n",
        "                loader = test_loader\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for inputs, labels in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.unsqueeze(labels, 1).type('torch.FloatTensor').to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, inputs)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            if phase == 'train':\n",
        "                fold_train_loss = running_loss / train_size\n",
        "                train_losses.append(fold_train_loss)\n",
        "                print(f'\\t\\ttrain loss: {fold_train_loss}')\n",
        "            else:\n",
        "                fold_val_loss = running_loss / test_size\n",
        "                val_losses.append(fold_val_loss)\n",
        "                print(f'\\t\\tval loss: {fold_val_loss}')\n",
        "\n",
        "        if epoch < warmup_epochs:\n",
        "            warmup_scheduler.step()\n",
        "        else:\n",
        "            main_scheduler.step()\n",
        "\n",
        "        if val_losses[epoch] < best_loss:\n",
        "            best_loss = val_losses[epoch]\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print()\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best mean val loss: {best_loss:.4f}')\n",
        "\n",
        "    model.load_state_dict(torch.load(best_model_params_path))\n",
        "\n",
        "    train_losses = np.array(train_losses)\n",
        "    val_losses = np.array(val_losses)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "CJAA1YqvJpqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huge_autoencoder = smp.Unet(\n",
        "    encoder_name=\"efficientnet-b2\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=3,\n",
        ")"
      ],
      "metadata": {
        "id": "lLYsjYjWT2aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_autoencoder = train_autoencoder(100)"
      ],
      "metadata": {
        "id": "4ZrppxrKCa2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape = dataset[0][0].shape\n",
        "autoencoder  = ConvAutoencoder(img_shape[1], img_shape[0], 1, 1)\n",
        "autoencoder.load_state_dict(torch.load('autoencoder_best_model_params.pt'))\n",
        "autoencoder  = autoencoder .to(device)"
      ],
      "metadata": {
        "id": "A9nEhkGnSyI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_random_autoencoder(ds : Dataset, autoenc):\n",
        "    idx = random.randint(0, len(ds)-1)\n",
        "    image, label = ds[idx]\n",
        "    x = image.unsqueeze(0).to(device)\n",
        "\n",
        "    autoenc.eval()\n",
        "    with torch.no_grad():\n",
        "        restored_image = autoenc(x).squeeze(0).cpu().detach().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=2, figsize=(8, 8), layout='tight')\n",
        "\n",
        "    image = np.swapaxes(image, 0, -1)\n",
        "    restored_image = np.swapaxes(restored_image, -1, 0)\n",
        "\n",
        "    axs[0].imshow(image)\n",
        "    axs[0].set_title('Original image')\n",
        "    axs[1].imshow(restored_image)\n",
        "    axs[1].set_title('Faked image')"
      ],
      "metadata": {
        "id": "PEFTWxheOZDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_random_autoencoder(test_dataset, last_autoencoder)"
      ],
      "metadata": {
        "id": "B9KJq60gPp4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting embeddings"
      ],
      "metadata": {
        "id": "b95Dm8pSXx3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(ds : Dataset, autoenc, idxs=None):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    if idxs is None:\n",
        "        for image, label in ds:\n",
        "            labels.append(label)\n",
        "\n",
        "            x = image.unsqueeze(0).to(device)\n",
        "            embed = autoenc.encoder(x).squeeze(0).cpu().detach().numpy()\n",
        "            embeddings.append(embed)\n",
        "    else:\n",
        "        for i in idxs:\n",
        "            image, label = ds[i]\n",
        "            labels.append(label)\n",
        "\n",
        "            x = image.unsqueeze(0).to(device)\n",
        "            embed = autoenc.encoder(x).squeeze(0).cpu().detach().numpy()\n",
        "            embeddings.append(embed)\n",
        "    return embeddings, labels"
      ],
      "metadata": {
        "id": "0-16jAcWYG5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings, labels = get_embeddings(test_dataset, autoencoder)"
      ],
      "metadata": {
        "id": "6yRqgKGiX7I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)"
      ],
      "metadata": {
        "id": "36ww77Tfcard"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_matrix = np.array(embeddings)\n",
        "labels = np.array(labels)\n",
        "\n",
        "pca_matrix = pca.fit_transform(embed_matrix)"
      ],
      "metadata": {
        "id": "GcnyN8DgahDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_examples = np.where(labels == 1)[0]\n",
        "normal_examples = np.where(labels == 0)[0]\n",
        "\n",
        "plt.scatter(pca_matrix[fake_examples, 0], pca_matrix[fake_examples, 1], color='red', label='fake')\n",
        "plt.scatter(pca_matrix[normal_examples, 0], pca_matrix[normal_examples, 1], color='blue', label='normal')\n",
        "plt.legend()\n",
        "plt.title('Train embeddings Normal/Fake')"
      ],
      "metadata": {
        "id": "p42CdB10co-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fishing_idxs = []\n",
        "true_idxs = []"
      ],
      "metadata": {
        "id": "lKBvh69eLLcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (_, label) in enumerate(train_dataset):\n",
        "    if label == 0:\n",
        "        true_idxs.append(i)\n",
        "    else:\n",
        "        fishing_idxs.append(i)"
      ],
      "metadata": {
        "id": "8uCRE0nEKrxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fishing_idxs = np.array(fishing_idxs)\n",
        "true_idxs = np.array(true_idxs)"
      ],
      "metadata": {
        "id": "fqphMhHQLcpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_single_embedding(ds : Dataset, autoenc, idx=None):\n",
        "    if idx is None:\n",
        "        idx = random.randint(0, len(ds)-1)\n",
        "    image, label = ds[idx]\n",
        "    x = image.unsqueeze(0).to(device)\n",
        "    embed = autoenc.encoder(x).squeeze(0).cpu().detach().numpy()\n",
        "    faked_image = autoenc(x).squeeze(0).cpu().detach().numpy()\n",
        "    return image, faked_image, embed, label\n",
        "\n",
        "def cos_distance(v1, v2):\n",
        "    return 1 - (np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
        "\n",
        "def analyse_embeds(ds : dataset, autoenc, relative_idx, k=30):\n",
        "    global true_idxs\n",
        "\n",
        "    image, restored_image, fake_embed, label = get_single_embedding(ds, autoenc, relative_idx)\n",
        "    all_embeddings, labels = get_embeddings(ds, autoenc)\n",
        "\n",
        "    distances = np.zeros(len(all_embeddings))\n",
        "    for i, v in enumerate(all_embeddings):\n",
        "        distances[i] = cos_distance(v, fake_embed)\n",
        "\n",
        "    sorted_idxs = np.argsort(distances)\n",
        "    distances = distances[sorted_idxs]\n",
        "\n",
        "    labels = np.array(labels)[sorted_idxs]\n",
        "    print('Distances', distances)\n",
        "    print()\n",
        "    print('True Labels', labels)\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    embed_matrix = np.array(all_embeddings)\n",
        "    labels = np.array(labels)\n",
        "    pca_matrix = pca.fit_transform(embed_matrix)\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=2, figsize=(16, 10), layout='tight')\n",
        "\n",
        "    image = np.swapaxes(image, 0, -1)\n",
        "    restored_image = np.swapaxes(restored_image, -1, 0)\n",
        "\n",
        "    axs[0].imshow(image)\n",
        "    axs[0].set_title('Original image')\n",
        "\n",
        "    fake_examples = np.where(labels == 1)[0]\n",
        "    normal_examples = np.where(labels == 0)[0]\n",
        "\n",
        "    axs[1].scatter(pca_matrix[fake_examples, 0], pca_matrix[fake_examples, 1], color='red', label='fake')\n",
        "    axs[1].scatter(pca_matrix[normal_examples, 0], pca_matrix[normal_examples, 1], color='blue', label='normal')\n",
        "\n",
        "    top_k = sorted_idxs[-1:-k-1:-1]\n",
        "    coords = pca_matrix[top_k, :]\n",
        "    for i, p1 in enumerate(coords):\n",
        "        p2 = pca_matrix[relative_idx, 0], pca_matrix[relative_idx, 1]\n",
        "        axs[1].plot([p1[0], p2[0]], [p1[1], p2[1]], '--', color='gray')\n",
        "        axs[1].text((p1[0]+p2[0])/2, (p1[1]+p2[1])/2, f'{distances[top_k[i]]:.2f}', color='black')\n",
        "\n",
        "\n",
        "    axs[1].scatter(pca_matrix[relative_idx, 0], pca_matrix[relative_idx, 1], color='black', label='picked fake')\n",
        "    axs[1].legend()\n",
        "    axs[1].set_title('PCA embeddings')"
      ],
      "metadata": {
        "id": "8_rCMViyLs0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_fishing_idx = np.random.choice(fishing_idxs)\n",
        "analyse_embeds(train_dataset, autoencoder, random_fishing_idx)"
      ],
      "metadata": {
        "id": "op8bQX8FLiPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    random_fishing_idx = np.random.choice(fishing_idxs)\n",
        "    analyse_embeds(train_dataset, autoencoder, random_fishing_idx)"
      ],
      "metadata": {
        "id": "wQf7Zb1viV1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}